{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQx5D0PQE5Gg"
      },
      "source": [
        "Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZJ_EDRgdE5h2"
      },
      "outputs": [],
      "source": [
        "# Q1.What is Simple Linear Regression?\n",
        "Ans:-simple linear regression (SLR) is a linear regression model with a single explanatory variable. That is, it concerns two-dimensional sample points with one independent\n",
        "variable and one dependent variable (conventionally, the x and y coordinates in a Cartesian coordinate system) and finds a linear function (a non-vertical straight line) that,\n",
        "as accurately as possible, predicts the dependent variable values as a function of the independent variable.\n",
        "\n",
        "# Q2.What are the key assumptions of Simple Linear Regression?\n",
        "Ans:-The key assumptions of Simple Linear Regression are that there's a linear relationship between the variables, the errors have a mean of zero and constant variance\n",
        "(homoscedasticity), the errors are normally distributed, and the errors are independent of each other and the predictors.\n",
        "1. Linearity:\n",
        "The relationship between the independent variable (X) and the dependent variable (Y) must be linear.\n",
        "This means that a straight line can adequately represent the relationship between the two variables.\n",
        "2. Independence of Errors:\n",
        "The error terms (residuals) must be independent of each other and the independent variable.\n",
        "This means that the error for one observation should not be related to the error for another observation or the value of the independent variable.\n",
        "3. Homoscedasticity:\n",
        "The variance of the error terms must be constant across all levels of the independent variable.\n",
        "In other words, the spread of the errors should be consistent regardless of the value of X.\n",
        "4. Normality of Errors:\n",
        "The error terms must be normally distributed.\n",
        "This assumption is particularly important when making inferences about the population, such as hypothesis testing or constructing confidence intervals.\n",
        "\n",
        "# Q3.What does the coefficient m represent in the equation Y=mX+c?\n",
        "Ans:-In the equation Y = mX + c, which represents the equation of a straight line, the coefficient m has the following interpretation:\n",
        "\n",
        "m: The Slope of the Line\n",
        "m is the slope of the line, and it describes how much Y changes for a one-unit change in X.\n",
        "Specifically, it tells you the rate of change or steepness of the relationship between the independent variable X and the dependent variable Y.\n",
        "\n",
        "Interpretation of the Slope:\n",
        "Positive slope (m > 0): As X increases, Y also increases. The relationship between X and Y is direct.\n",
        "Negative slope (m < 0): As X increases, Y decreases. The relationship between X and Y is inverse.\n",
        "Zero slope (m = 0): There is no change in Y as X changes. Y is constant.\n",
        "\n",
        "# Q4.What does the intercept c represent in the equation Y=mX+c?\n",
        "Ans:-What does it represent?\n",
        "c is the value of Y when X = 0.\n",
        "It's the point where the line crosses the Y-axis on a graph.\n",
        "\n",
        "Interpretation:\n",
        "It gives you a baseline value of Y when there's no contribution from X (i.e., when X = 0).\n",
        "It can be meaningful or just a mathematical artifact, depending on the context.\n",
        "\n",
        "Example:\n",
        "Let's say we have a regression equation:\n",
        "\n",
        "               Y=2X+5\n",
        "hence,c=5\n",
        "This means when X = 0, Y = 5.\n",
        "On a graph, the line crosses the Y-axis at (0, 5).\n",
        "\n",
        "# Q5. How do we calculate the slope m in Simple Linear Regression?\n",
        "Ans:-linear regression, the slope m represents the change in the dependent variable (Y) for each unit change in the independent variable (X).\n",
        "It's calculated using the formula: m = Î£(xi - xÌ„)(yi - È³) / Î£(xi - xÌ„)Â², where xi and yi are individual data points, xÌ„ and È³ are the means of X and Y,\n",
        "and the summations are over all data points.\n",
        "1. Calculate the means:\n",
        "Find the average of all X values (xÌ„) and the average of all Y values (È³).\n",
        "2. Calculate the deviations:\n",
        "For each data point, subtract the mean of X (xÌ„) from the X value (xi) and the mean of Y (È³) from the Y value (yi).\n",
        "3. Calculate the products of deviations:\n",
        "Multiply the deviation of each X value (xi - xÌ„) by the corresponding deviation of the Y value (yi - È³).\n",
        "4. Calculate the squared deviations:\n",
        "For each data point, square the deviation of the X value (xi - xÌ„)Â².\n",
        "5. Sum the products and squared deviations:\n",
        "Add up all the products of deviations calculated in step 3 and all the squared deviations calculated in step 4.\n",
        "6. Calculate the slope:\n",
        "Divide the sum of the products of deviations (Î£(xi - xÌ„)(yi - È³)) by the sum of the squared deviations (Î£(xi - xÌ„)Â²). The result is the slope, m.\n",
        "\n",
        "The formula can also be expressed as:\n",
        "m = [n * Î£(xy) - Î£(x) * Î£(y)] / [n * Î£(xÂ²) - (Î£(x))Â²]\n",
        "where:\n",
        "n is the number of data points.\n",
        "Î£(xy) is the sum of the product of each x and y value.\n",
        "Î£(x) is the sum of all x values.\n",
        "Î£(y) is the sum of all y values.\n",
        "Î£(xÂ²) is the sum of the squares of all x values.\n",
        "\n",
        "# Q6.What is the purpose of the least squares method in Simple Linear Regression?\n",
        "Ans:-The purpose of the least squares method in simple linear regression is to find the line of best fit that minimizes the sum of the squared differences between\n",
        "the observed data points and the corresponding points on the line. This line, also known as the regression line, is used to estimate the relationship between an\n",
        "independent and dependent variable.\n",
        "The least squares method is a form of mathematical regression analysis used to determine the line of best fit for a set of data, providing a visual demonstration\n",
        "of the relationship between the data points. Each point of data represents the relationship between a known independent variable and an unknown dependent variable.\n",
        "\n",
        "# Q7.How is the coefficient of determination (RÂ²) interpreted in Simple Linear Regression?\n",
        "Ans:-In simple linear regression, the coefficient of determination (RÂ²) is interpreted as the proportion of the variance in the dependent variable that is predictable\n",
        "from the independent variable. It essentially measures how well the regression line fits the data, indicating the strength of the relationship between the variables.\n",
        "R2 tells you how well your linear regression model explains the variation in the dependent variable y.\n",
        "\n",
        "                          Total Variation\n",
        "                     R2=  Explained Variation\n",
        "  It ranges from 0 to 1.\n",
        "\n",
        "Interpretation Examples:\n",
        "RÂ² = 0.70 means that 70% of the variance in the dependent variable is explained by the independent variable.\n",
        "RÂ² = 0.25 means that only 25% of the variance in the dependent variable is explained by the independent variable.\n",
        "\n",
        "# Q8.What is Multiple Linear Regression?\n",
        "Ans:-Multiple linear regression is a statistical technique used to predict the relationship between one dependent variable and two or more independent variables.\n",
        "It aims to find a linear relationship between these variables, allowing for more precise predictions than simple linear regression, which only considers one independent variable.\n",
        "This method is widely used in various fields like economics, finance, and social sciences to forecast outcomes, identify patterns, and understand the combined influence of\n",
        "multiple factors on a single result.\n",
        "\n",
        "# Q9.What is the main difference between Simple and Multiple Linear Regression?\n",
        "Ans:-Simple Linear Regression:\n",
        "Definition:\n",
        "A statistical method that explores the relationship between one dependent variable and one independent variable.\n",
        "Model:\n",
        "The relationship is represented by a linear equation: Y = C0 + C1X + e, where Y is the dependent variable, X is the independent variable, C0 and C1 are coefficients, and e is the error term.\n",
        "Purpose:\n",
        "To understand how one independent variable influences the dependent variable.\n",
        "Example:\n",
        "Predicting house prices based on square footage alone.\n",
        "\n",
        "Multiple Linear Regression:\n",
        "Definition:\n",
        "A statistical method that explores the relationship between one dependent variable and two or more independent variables.\n",
        "Model:\n",
        "The relationship is represented by a linear equation: Y = C0 + C1X1 + C2X2 + ... + CnXn + e, where Y is the dependent variable, X1, X2, ..., Xn are the independent variables, C0, C1, C2, ..., Cn are coefficients, and e is the error term.\n",
        "Purpose:\n",
        "To understand how multiple independent variables, collectively, influence the dependent variable.\n",
        "Example:\n",
        "Predicting house prices based on square footage, number of bedrooms, and location.\n",
        "\n",
        "# Q10.What are the key assumptions of Multiple Linear Regression?\n",
        "Ans:-Multiple linear regression relies on several key assumptions for accurate and reliable results. These include linearity, independence of observations, homoscedasticity (constant variance of errors), normality of residuals, and the absence of multicollinearity.\n",
        "Here's a more detailed breakdown:\n",
        "\n",
        "Linearity:\n",
        "The relationship between the dependent variable and each independent variable should be linear, meaning it can be represented by a straight line. This can be checked visually using scatterplots.\n",
        "\n",
        "Independence of Observations:\n",
        "Each data point (observation) should be independent of the others, meaning there is no pattern or correlation between them. This can be checked by ensuring that there are no repeated or related observations in the dataset.\n",
        "\n",
        "Homoscedasticity (Constant Variance of Errors):\n",
        "The variance of the errors (residuals) should be constant across all levels of the independent variables. This means the spread of the residuals should be roughly the same, regardless of the values of the independent variables.\n",
        "\n",
        "Normality of Residuals:\n",
        "The errors (residuals) should be approximately normally distributed. This can be assessed using histograms or Q-Q plots.\n",
        "\n",
        "No Multicollinearity:\n",
        "The independent variables should not be highly correlated with each other. If independent variables are highly correlated, it can be difficult to determine the individual effects of each variable on the dependent variable.\n",
        "\n",
        "# Q11.What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?\n",
        "Ans:-Heteroscedasticity in a Multiple Linear Regression model refers to a situation where the variance of the error terms (residuals) is not constant across different values of the independent variables. This violation of a key assumption of Ordinary Least Squares (OLS) regression can lead to inaccurate standard errors, invalid hypothesis tests, and potentially biased coefficient estimates.\n",
        "\n",
        "How it affects Multiple Linear Regression:\n",
        "Unreliable Standard Errors:\n",
        "The standard errors of the regression coefficients are typically used to assess the significance of the independent variables. In the presence of heteroscedasticity, these standard errors become biased, meaning they do not accurately reflect the true uncertainty in the coefficient estimates.\n",
        "\n",
        "Invalid Hypothesis Tests:\n",
        "Standard hypothesis tests, like t-tests and F-tests, rely on the assumption of constant error variance. When heteroscedasticity is present, these tests are invalid, and the conclusions drawn from them may be misleading.\n",
        "\n",
        "Potential Bias in Coefficient Estimates:\n",
        "While the Ordinary Least Squares (OLS) estimator remains unbiased in the presence of heteroscedasticity, it is no longer the most efficient estimator. This means that the coefficient estimates might be less precise than they would be if the errors were homoscedastic.\n",
        "\n",
        "Inaccurate Confidence Intervals:\n",
        "Confidence intervals constructed using the biased standard errors will be wider than they should be, leading to inaccurate estimates of the true parameter values.\n",
        "\n",
        "Difficulty in Interpreting Results:\n",
        "The presence of heteroscedasticity can make it challenging to interpret the relationships between the independent and dependent variables because the uncertainty surrounding the estimated coefficients is not correctly captured.\n",
        "\n",
        "# Q12.How can you improve a Multiple Linear Regression model with high multicollinearity?\n",
        "Ans:-To improve a Multiple Linear Regression model with high multicollinearity, consider these strategies: feature selection, Principal Component Analysis (PCA), regularization techniques (Ridge and Lasso regression), and data collection. Additionally, creating interaction terms or expanding your dataset can also help mitigate the effects of multicollinearity.\n",
        "\n",
        "Detailed Explanation:\n",
        "\n",
        "1. Feature Selection:\n",
        "Identify and remove highly correlated variables from the model. This can be done by examining correlation matrices or using techniques like the Variance Inflation Factor (VIF).\n",
        "\n",
        "2. Principal Component Analysis (PCA):\n",
        "PCA transforms correlated predictors into a smaller set of uncorrelated components, capturing the essential variance in the data.\n",
        "\n",
        "3. Regularization Techniques:\n",
        "Ridge Regression: Penalizes large coefficients, shrinking them towards zero and stabilizing parameter estimates.\n",
        "Lasso Regression: Similar to Ridge, but it can also set some coefficients to zero, effectively removing variables from the model.\n",
        "\n",
        "4. Data Collection:\n",
        "Increase the sample size or collect new variables that are less correlated with existing predictors.\n",
        "\n",
        "5. Interaction Terms:\n",
        "Introduce interaction terms between correlated predictors to capture their combined effects on the outcome variable.\n",
        "\n",
        "6. Data Centering:\n",
        "Center the variables by subtracting their mean from each observation. This can reduce correlation between variables.\n",
        "\n",
        "7. Combining Variables:\n",
        "If correlated variables measure similar concepts, consider creating a new variable that combines them.\n",
        "\n",
        "# Q13.What are some common techniques for transforming categorical variables for use in regression models?\n",
        "Ans:-Several techniques transform categorical variables for use in regression models. One-hot encoding, also known as dummy encoding, creates a binary variable for each category, representing the presence or absence of that category. Label encoding assigns unique numerical values to each category, which is useful for ordinal variables with a natural order, according to LinkedIn. Other techniques include ordinal encoding, which allows explicit mapping between categories and integers for ordinal variables, and frequency encoding, which replaces categories with their frequency of occurrence.\n",
        "\n",
        "1.One-Hot Encoding (Dummy Encoding):\n",
        "This method creates a new binary column for each category of the original variable.\n",
        "Each binary column has a value of 1 if the observation belongs to that category and 0 otherwise.\n",
        "It's particularly useful for nominal variables where no inherent order exists.\n",
        "\n",
        "2.Label Encoding:\n",
        "Assigns a unique numerical value to each category.\n",
        "It's suitable for ordinal variables where there's a natural order, but can introduce unintended ordinal relationships if used on nominal data.\n",
        "\n",
        "3.Ordinal Encoding:\n",
        "Similar to label encoding, but allows explicit mapping between categories and integer labels.\n",
        "Useful when a clear and predefined ordinal relationship exists.\n",
        "\n",
        "4.Frequency Encoding:\n",
        "Replaces each category with its frequency or occurrence in the dataset.\n",
        "Captures the distribution and importance of different categories.\n",
        "\n",
        "# Q14.What is the role of interaction terms in Multiple Linear Regression?\n",
        "Ans:-In multiple linear regression, interaction terms reveal how the effect of one independent variable on the dependent variable changes depending on the value of another independent variable. They essentially capture non-additive relationships between variables, allowing for a more nuanced understanding of how predictors influence the outcome.\n",
        "\n",
        "Elaboration:\n",
        "Identifying Non-Additive Relationships:\n",
        "Interaction terms are crucial for identifying situations where the effect of one variable on the outcome is not constant but rather changes based on the level of another variable. For example, the impact of advertising spending on sales might depend on the time of year (e.g., a higher effect during holiday seasons).\n",
        "\n",
        "Flexibility in Modeling:\n",
        "By including interaction terms, the model can accommodate different slopes for different lines, leading to a potentially better fit to the data and improved predictive performance.\n",
        "\n",
        "Interpreting Coefficients:\n",
        "The presence of an interaction term can complicate the interpretation of the main effect coefficients (coefficients of individual variables without interactions), as the impact of each variable is now conditional on the value of the other.\n",
        "\n",
        "Quantifying the Interaction Effect:\n",
        "In most regression models, interaction effects are quantified as the product of two associated explanatory variables (e.g., X1 * X2). The coefficient of this interaction term (Î²) indicates the extent to which the relationship between the dependent variable and X1 is influenced by X2.\n",
        "\n",
        "Importance in Research:\n",
        "Understanding interactions is essential for gaining a deeper understanding of the relationships between predictors and the response variable, and for preventing misleading interpretations of the main effects. For example, in a study of employee satisfaction, there might be an interaction between job autonomy and manager support, where autonomy has a larger positive impact on satisfaction when manager support is high.\n",
        "\n",
        "# Q15.How can the interpretation of intercept differ between Simple and Multiple Linear Regression?\n",
        "Ans:-In both simple and multiple linear regression, the intercept represents the predicted value of the dependent variable (Y) when all independent variables are zero.\n",
        "However, the interpretation of the intercept can be more nuanced in multiple linear regression due to the presence of multiple independent variables. In simple regression,\n",
        "the intercept is the predicted value when the single independent variable (X) is zero. In multiple regression, the intercept is the predicted value when all independent variables\n",
        "(X1, X2, ..., Xn) are zero.\n",
        "\n",
        "Simple Linear Regression\n",
        "Intercept: The predicted value of Y when X = 0. If X = 0 is a meaningful value in the context of your data, the intercept has a straightforward interpretation. For example, if you are predicting sales (Y) based on advertising spend (X), the intercept could represent the expected sales if no advertising is done.\n",
        "Slope: The change in Y for each unit increase in X.\n",
        "\n",
        "Multiple Linear Regression\n",
        "Intercept:\n",
        "The predicted value of Y when all independent variables (X1, X2, ..., Xn) = 0. This interpretation is the same as in simple linear regression. However, the meaning of this intercept can be more complex because it's not necessarily a realistic scenario for all data.\n",
        "Slopes:\n",
        "Each slope coefficient (Î²1, Î²2, ..., Î²n) represents the change in Y for a one-unit increase in the corresponding independent variable (X1, X2, ..., Xn), holding all other independent variables constant. This is the key difference in interpretation.\n",
        "\n",
        "Differences in Interpretation\n",
        "1. Context of the Independent Variables:\n",
        "In multiple regression, the intercept is the predicted value when all independent variables are zero. If some or all of your independent variables are categorical, this scenario might not be relevant in real-world conditions. For example, if one of your independent variables is gender (binary, 0 or 1), then the intercept might not have a practical meaning.\n",
        "2. Control for Other Variables:\n",
        "The slopes in multiple regression allow you to assess the impact of each independent variable while controlling for the effects of other independent variables. This is a significant advantage of multiple regression over simple regression.\n",
        "3. Realistic Scenarios:\n",
        "In multiple regression, the intercept might not be a realistic scenario. For example, if you are predicting house prices, and one of your independent variables is the number of bedrooms, the intercept might not represent a realistic house with zero bedrooms.\n",
        "4.essence:\n",
        "Simple linear regression provides a straightforward interpretation of the intercept and slope.\n",
        "Multiple linear regression allows for more complex and realistic interpretations of the slopes, while the intercept might require more careful consideration, especially if the independent variables are not inherently zero.\n",
        "\n",
        "# Q16.What is the significance of the slope in regression analysis, and how does it affect predictions?\n",
        "Ans:-The slope represents the rate of change in the dependent variable for every one-unit change in the independent variable. It's a crucial parameter that directly influences the accuracy and interpretation of predictions made using the regression model.\n",
        "1. Significance of the Slope:\n",
        "Rate of Change:\n",
        "The slope indicates how much the predicted value of the dependent variable (Y) changes when the independent variable (X) changes by one unit.\n",
        "Direction of Relationship:\n",
        "A positive slope indicates a positive relationship (as X increases, Y tends to increase), while a negative slope indicates a negative relationship (as X increases, Y tends to decrease).\n",
        "Predictive Power:\n",
        "The slope, along with the intercept, defines the linear relationship between the variables, allowing for the estimation of average rate of change and predictions.\n",
        "\n",
        "2. How the Slope Affects Predictions:\n",
        "Accuracy:\n",
        "A larger (absolute) slope value indicates a steeper line, implying a stronger relationship and potentially more accurate predictions.\n",
        "Bias:\n",
        "If the slope is significantly different from zero, it indicates that the independent variable has a significant influence on the dependent variable, and the model is less likely to be biased in its predictions.\n",
        "Interpretation:\n",
        "The slope provides insights into how changes in the independent variable impact the dependent variable, allowing for more informed interpretations of the regression model's predictions.\n",
        "\n",
        "# Q17.How does the intercept in a regression model provide context for the relationship between variables?\n",
        "Ans:-The intercept in a regression model provides crucial context by representing the expected value of the dependent variable when all independent variables are zero.\n",
        "It acts as a baseline or starting point, indicating the value of the dependent variable when no other factors are in play. In essence, it helps to understand the relationship\n",
        "between variables by establishing a reference point for the regression line.\n",
        "In a simple linear regression model:\n",
        "                          Y=Î²0+Î²1X+Ïµ\n",
        "ð›½0is the intercept\n",
        "ð›½1is the slope (effect of X on Y)\n",
        "ðœ–is the error term\n",
        "\n",
        "What the intercept tells you:\n",
        "It gives a baseline value of Y.\n",
        "It helps \"anchor\" the regression line within the data space.\n",
        "It may or may not have practical interpretability, depending on whether it makes sense for all X's to be zero.\n",
        "\n",
        "Example 1: Interpretable intercept\n",
        "Say you're modeling income based on years of education:\n",
        "                       Income=Î²0+Î²1XEducation\n",
        "\n",
        "Ifð›½0=20,000, it implies someone with zero years of education is predicted to earn $20,000. Thatâ€™s interpretable (if not realistic).\n",
        "\n",
        "Example 2: Intercept is not meaningful\n",
        "If youâ€™re modeling test scores based on hours studied and hours slept, then:\n",
        "                    Score=Î²0+Î²1XStudyHours+Î²2XSleepHour\n",
        "\n",
        "the intercept is the expected test score if a person didnâ€™t study and didnâ€™t sleep.\n",
        "\n",
        "# Q18.What are the limitations of using RÂ² as a sole measure of model performance?\n",
        "Ans:-RÂ² is a useful metric for evaluating regression models, but relying solely on it can be misleading. While it measures the proportion of variance in the dependent variable explained by the model, it doesn't tell the whole story about model quality or predictive accuracy. RÂ² can be inflated by adding irrelevant variables (overfitting), doesn't capture non-linear relationships well, and is not comparable across datasets with different variances.\n",
        "Here's a more detailed look at the limitations:\n",
        "\n",
        "1. Doesn't Guarantee a Good Model:\n",
        "A high RÂ² doesn't mean the model is reliable or that the chosen regression is appropriate.\n",
        "It only measures the variance explained, not the model's overall goodness-of-fit or predictive ability.\n",
        "A model with a low RÂ² can still be useful if it accurately captures the underlying relationships in the data, even if it doesn't explain all the variance.\n",
        "\n",
        "2. Sensitive to Unnecessary Features and Overfitting:\n",
        "RÂ² will increase even with the addition of irrelevant variables, leading to overfitting, where the model learns noise from the training data.\n",
        "This can result in a model that performs well on the training data but poorly on new, unseen data.\n",
        "Adjusted RÂ² helps mitigate this by penalizing models with more predictors.\n",
        "\n",
        "3. Doesn't Capture Non-Linear Relationships:\n",
        "RÂ² is primarily designed for linear regression models and may not accurately reflect the performance of models that capture non-linear relationships.\n",
        "Alternative metrics, like RMSE (Root Mean Squared Error) or MAE (Mean Absolute Error), may be more appropriate for non-linear models.\n",
        "\n",
        "4. Not Comparable Across Different Datasets:\n",
        "RÂ² values are dataset-specific and not directly comparable across different studies or contexts.\n",
        "The variance of the dependent variable can affect the RÂ² value, making comparisons between datasets with different variances difficult.\n",
        "\n",
        "5. Ignores the Size of Coefficients and Causation:\n",
        "RÂ² doesn't indicate whether the estimated coefficients are large enough to be practically significant.\n",
        "A high RÂ² doesn't imply a causal relationship between the variables.\n",
        "\n",
        "# Q19.How would you interpret a large standard error for a regression coefficient?\n",
        "Ans:-A large standard error for a regression coefficient suggests that the estimated coefficient is likely to be a less precise estimate of the true population value.\n",
        "It indicates a higher level of variability in the estimated coefficient if the regression were to be run on different samples of the same data. In other words,\n",
        "the coefficient estimate is more likely to fluctuate if the model were re-estimated on different datasets.\n",
        "\n",
        "What is a standard error?\n",
        "The standard error (SE) quantifies the variability of a statistic (like a regression coefficient) across different samples.\n",
        "It tells you how much the estimated coefficient would \"bounce around\" if you repeated the study multiple times.\n",
        "A smaller SE implies a more precise estimate, meaning the estimated coefficient is more likely to be close to the true population value.\n",
        "\n",
        "# Q20.How can heteroscedasticity be identified in residual plots, and why is it important to address it?\n",
        "Ans:-Heteroscedasticity, or unequal error variance, in residual plots is identified by patterns like a \"funnel\" or \"cone\" shape, where the spread of residuals increases or decreases as the fitted values increase. It's important to address because it violates assumptions of linear regression, leading to inaccurate standard errors, biased coefficient estimates, and unreliable statistical tests.\n",
        "\n",
        "How to Identify Heteroscedasticity:\n",
        "1. Residuals vs. Fitted Values Plot:\n",
        "Plot the residuals against the predicted values (fitted values) from the model. A random scatter indicates homoscedasticity (equal variance). A funnel or cone shape suggests heteroscedasticity.\n",
        "\n",
        "2. Scale-Location Plot:\n",
        "Plot the square root of the absolute residuals against the fitted values. This can help visualize the variance of the residuals and detect patterns more easily.\n",
        "\n",
        "3. Residuals vs. Predictor Plots:\n",
        "Plot the residuals against each predictor variable. Any systematic pattern or unequal spread of residuals across the range of predictor values can indicate heteroscedasticity.\n",
        "\n",
        "# Q21.What does it mean if a Multiple Linear Regression model has a high RÂ² but low adjusted RÂ²?\n",
        "Ans:-If a multiple linear regression model has a high RÂ² but a low adjusted RÂ², it suggests that the model may be overfitting the data.\n",
        "The inclusion of additional independent variables, even if they don't significantly improve predictive power, will always increase the RÂ².\n",
        "The adjusted RÂ², however, penalizes the inclusion of irrelevant predictors. A high RÂ² with a low adjusted RÂ² indicates that the added variables are not\n",
        "contributing meaningfully to the model's explanatory power, and the model is likely too complex for the given data.\n",
        "This happens when you've included too many irrelevant predictors, causing the RÂ² to inflate artificially, even though the model's predictive power for new data\n",
        "is not significantly improved.\n",
        "\n",
        "# Q22.Why is it important to scale variables in Multiple Linear Regression?\n",
        "Ans:-Scaling variables in Multiple Linear Regression is important because it ensures that all variables contribute equally to the model, preventing variables with larger magnitudes from dominating the analysis. It also simplifies calculations, improves the interpretability of coefficients, and is crucial for certain regularization techniques like Lasso and Ridge Regression.\n",
        "Here's a more detailed explanation:\n",
        "1. Preventing Dominance by Large Variables:\n",
        "Without scaling, variables with larger magnitudes (e.g., thousands vs. hundreds) can disproportionately influence the model, potentially masking the influence of other variables.\n",
        "Scaling brings all variables to a common scale, allowing each variable to contribute more fairly to the model's results.\n",
        "2. Simpler Calculations and Interpretation:\n",
        "Scaling can simplify calculations, especially when dealing with sums or averages of variables on different scales.\n",
        "Standardizing variables (e.g., Z-score scaling) can make the intercept term more interpretable, representing the expected value of the dependent variable when all independent variables are at their means.\n",
        "3. Importance for Regularization Techniques:\n",
        "Techniques like Lasso and Ridge Regression rely on penalties that are applied based on the magnitude of coefficients.\n",
        "Without scaling, these penalties might disproportionately favor or penalize variables with larger scales, leading to biased results.\n",
        "4. Numerical Stability and Accuracy:\n",
        "When dealing with large numbers or very small numbers, computers can have difficulty with precise calculations. Scaling can improve numerical stability and prevent potential inaccuracies.\n",
        "5. Faster Convergence of Algorithms:\n",
        "Scaling can help algorithms like Gradient Descent converge faster to a solution, especially when dealing with different scales of variables.\n",
        "\n",
        "# Q23.What is polynomial regression?\n",
        "Ans:-Polynomial regression is a form of regression analysis where the relationship between a predictor variable (x) and a response variable (y) is modeled using a polynomial\n",
        "equation of degree. It extends linear regression by allowing for non-linear relationships, capturing curved patterns in data.\n",
        "\n",
        "Non-linear Relationships:\n",
        "Unlike linear regression, which assumes a straight-line relationship, polynomial regression models non-linear relationships, such as curves or bends, in the data.\n",
        "\n",
        "Polynomial Equations:\n",
        "The relationship is modeled using a polynomial equation, which can include terms like x, xÂ², xÂ³, and so on, where 'n' is the degree of the polynomial.\n",
        "\n",
        "Flexibility:\n",
        "Polynomial regression offers more flexibility than linear regression in modeling complex relationships, potentially leading to better predictive accuracy, according to Applied AI Course.\n",
        "\n",
        "Real-world Applications:\n",
        "It's used in various fields like finance, biology, and physics, where non-linear patterns are common, says Applied AI Course.\n",
        "\n",
        "Supervised Learning:\n",
        "Polynomial regression is a method of supervised learning, where the model learns from labeled data to make predictions, states Applied AI Course.\n",
        "\n",
        "Degree of the Polynomial:\n",
        "The degree of the polynomial equation (the highest power of x in the equation) can be adjusted based on the complexity of the data and the desired level of model fit.\n",
        "\n",
        "# Q24.How does polynomial regression differ from linear regression?\n",
        "Ans:-Polynomial regression extends linear regression by allowing for non-linear relationships between variables, while linear regression assumes a straight-line relationship.\n",
        "Polynomial regression achieves this by adding higher-degree terms (like xÂ², xÂ³, etc.) to the model, which allows the model to fit curved lines to the data.\n",
        "                Linear Regression                                     Polynomial Regression\n",
        "                1.Assumes a straight-line relationship                 Models a curved relationship\n",
        "                2.Y=Î²0+Î²1XY                                            Y=Î²0+Î²1X+Î²2X2+â‹¯+Î²nXnY\n",
        "                3.Linear in X                                         Linear in coefficients, but nonlinear in X\n",
        "                4.Limited (only fits straight lines)                   More flexible (can fit curves of various shapes)\n",
        "                5.Less prone to overfitting                           Can overfit if the degree is too high\n",
        "                6.Straight line                                       Curved line (U-shape, wave, etc.)\n",
        "\n",
        "Linear Regression: Uses a straight line to model the relationship between variables.\n",
        "Polynomial Regression: Uses a curved line to model the relationship, allowing for more complex patterns.\n",
        "\n",
        "# Q25.- When is polynomial regression used?\n",
        "Ans:-Polynomial regression is used when the relationship between variables is non-linear and cannot be accurately captured by a simple linear model.\n",
        "It's essentially an extension of linear regression that allows for curved lines to be fit to the data. This technique is useful in various fields like\n",
        "finance, healthcare, and machine learning when analyzing complex systems with non-linear relationships.\n",
        "Elaboration:\n",
        "1.Non-linear relationships:\n",
        "When data points do not form a straight line when plotted, polynomial regression can be used to find a curved line (polynomial) that best fits the data.\n",
        "2.Extension of linear regression:\n",
        "It builds upon the principles of linear regression but introduces polynomial terms to model non-linear relationships.\n",
        "3.Applications:\n",
        "Finance: Modeling stock trends and predicting future prices.\n",
        "Healthcare: Predicting growth patterns and analyzing patient data.\n",
        "Machine learning: Capturing complex non-linear relationships in datasets.\n",
        "Engineering and Physics: Analyzing complex systems and processes with non-linear behavior.\n",
        "\n",
        "# Q26.What is the general equation for polynomial regression?\n",
        "Ans:-The general equation for polynomial regression is a polynomial function where the relationship between the independent variable (X) and the dependent variable (Y) is modeled\n",
        "as a polynomial of degree 'n'. It takes the form: *n* is:\n",
        "\n",
        "y = Î²â‚€ + Î²â‚x + Î²â‚‚xÂ² + Î²â‚ƒxÂ³ + ... + Î²â‚™xâ¿ + Îµ\n",
        "\n",
        "where:\n",
        "\n",
        "*   y is the dependent variable.\n",
        "*   x is the independent variable.\n",
        "*   Î²â‚€, Î²â‚, Î²â‚‚, ..., Î²â‚™ are the coefficients of the polynomial (constants to be estimated).\n",
        "*   n is the degree of the polynomial (determines the \"curviness\").\n",
        "*   Îµ is the error term, representing the difference between the observed value and the predicted value.\n",
        "\n",
        "# Q27.Can polynomial regression be applied to multiple variables?\n",
        "Ans:-Yes, polynomial regression can be applied to multiple variables.  This is often referred to as **multiple polynomial regression**.  Instead of fitting a polynomial to a single\n",
        "independent variable, you fit a polynomial function of multiple independent variables.  The complexity increases significantly, as the number of terms in the polynomial grows rapidly\n",
        "with the number of variables and the degree of the polynomial.\n",
        "Here's how it works:\n",
        "1. Transforming variables:\n",
        "You take each independent variable and create new variables by raising them to different powers (e.g., x, xÂ², xÂ³, x*y, etc.).\n",
        "2. Building the model:\n",
        "You then treat these new variables as if they were separate independent variables in a multiple linear regression model.\n",
        "3. Capturing interactions:\n",
        "This approach allows you to capture not only the individual effects of each variable on the outcome but also potential interactions between them.\n",
        "\n",
        "# Q28.What are the limitations of polynomial regression?\n",
        "Ans:-Polynomial regression, while flexible for modeling non-linear relationships, has limitations like overfitting, high computational complexity, and difficulty in interpreting\n",
        "high-degree polynomials. It's also sensitive to outliers and may not extrapolate well outside the range of observed data.\n",
        "Here's a more detailed look at the limitations:\n",
        "1. Overfitting:\n",
        "Risk:\n",
        "Higher-degree polynomials can fit the training data very closely, including noise and random variations, leading to a model that performs poorly on new, unseen data.\n",
        "Consequences:\n",
        "This means the model might memorize the training data instead of learning the underlying relationship, resulting in inaccurate predictions on new data.\n",
        "2. Computational Complexity:\n",
        "Increase in Complexity:\n",
        "As the degree of the polynomial increases, so does the number of parameters to estimate, making the model more complex and computationally expensive.\n",
        "Impact:\n",
        "Training high-degree polynomial models can be slower, especially with large datasets, requiring more resources and time.\n",
        "3. Interpretability:\n",
        "Difficulty:\n",
        "High-degree polynomials can be challenging to interpret and explain, making it harder to understand how the model is making predictions.\n",
        "Contrast:\n",
        "Linear models are easier to interpret, as the coefficients directly represent the relationship between variables.\n",
        "4. Extrapolation:\n",
        "Poor Extrapolation:\n",
        "Polynomial models may not accurately predict values outside the range of the observed data, especially with high-degree polynomials.\n",
        "Issue:\n",
        "High-degree polynomials can exhibit erratic behavior outside the data range, making them unsuitable for making predictions about data far from the training set.\n",
        "5. Sensitivity to Outliers:\n",
        "Impact:\n",
        "Outliers can significantly influence the results of polynomial regression, potentially leading to a distorted model.\n",
        "Comparison:\n",
        "Linear regression is less sensitive to outliers compared to polynomial regression, as it focuses on the overall trend rather than individual data points.\n",
        "6. Data Requirements:\n",
        "Need for Data:\n",
        "Polynomial regression, especially with higher-degree polynomials, requires a sufficient amount of data to estimate the complex polynomial terms reliably.\n",
        "Small Datasets:\n",
        "Small datasets may not provide enough information to capture the underlying relationships accurately, potentially leading to overfitting or underfitting.\n",
        "7. Model Selection:\n",
        "Choosing the right degree:\n",
        "Determining the appropriate degree for the polynomial is a challenge, as a low-degree polynomial may underfit the data while a high-degree polynomial risks overfitting.\n",
        "Techniques:\n",
        "Techniques like cross-validation are often needed to find the optimal degree, adding complexity to the process.\n",
        "\n",
        "# Q29.What methods can be used to evaluate model fit when selecting the degree of a polynomial?\n",
        "Ans:-Several methods can be used to evaluate model fit when selecting the degree of a polynomial, including visual inspection, splitting data into training and validation/test sets, and using metrics like R-squared and Mean Squared Error (MSE).\n",
        "1.Visual Inspection:\n",
        "* Plotting the data and the fitted polynomial curves can help visualize the fit and identify potential underfitting or overfitting. LinkedIn suggests this method.\n",
        "* A well-fitted curve should closely follow the data points without overly complex curves or excessive oscillations.\n",
        "2.Data Splitting and Validation:\n",
        "* Training Set: Used to fit the polynomial model for each degree.\n",
        "* Validation Set: Used to evaluate the model's performance for each degree and helps identify overfitting.\n",
        "* Test Set: Used for a final, unbiased evaluation of the chosen model after selecting the optimal degree based on the validation set.\n",
        "3.Model Evaluation Metrics:\n",
        "* R-squared:\n",
        "Measures the proportion of variance in the dependent variable that is predictable from the independent variable(s). A higher R-squared indicates a better fit.\n",
        "4.Mean Squared Error (MSE):\n",
        "Measures the average squared difference between predicted and actual values. A lower MSE indicates a better fit.\n",
        "* Cross-validation:\n",
        "Splitting the data into multiple folds and using each fold for validation while training on the remaining folds to get a more robust estimate of the model's performance.\n",
        "\n",
        "# Q30.Why is visualization important in polynomial regression?\n",
        "Ans:-Visualization is crucial in polynomial regression because it helps identify when a linear model is insufficient and a curved relationship is present, allowing for more accurate model fitting. By visualizing the data and the polynomial regression curve, one can assess how well the model fits the data, detect potential overfitting, and understand the nature of the relationship between variables.\n",
        "Here's why visualization is so important:\n",
        "1.Revealing Curvature:\n",
        "A simple scatter plot can highlight the curvature or non-linearity in the data, suggesting the need for polynomial regression.\n",
        "2.Assessing Model Fit:\n",
        "Comparing the polynomial regression curve to the actual data points visually shows how well the model captures the underlying relationship.\n",
        "3.Detecting Overfitting:\n",
        "Overly complex polynomial models (high degree) can overfit the training data, leading to a curve that fits the noise rather than the underlying trend. Visualization helps identify this by showing a curve that oscillates too much or deviates significantly from the general trend, according to Analytics Vidhya and Number Analytics.\n",
        "4.Understanding Nonlinear Relationships:\n",
        "Polynomial regression models are designed to capture complex curves, and visualization allows for a clear understanding of the nature of the relationship between variables.\n",
        "5.Comparing Models:\n",
        "Visualizing the fit of different polynomial degrees helps in choosing the appropriate model complexity to balance fit and generalization, according to Analytics Vidhya and LinkedIn.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 524
        },
        "id": "7_2sB6uXqYU1",
        "outputId": "e7ba0fa0-d015-40b8-f426-f09676630b9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "R-squared: nan\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
            "  warnings.warn(msg, UndefinedMetricWarning)\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#Q31.How is polynomial regression implemented in Python?\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score\n",
        "# Assuming you have your data in a pandas DataFrame called 'df'\n",
        "# with 'x' as the independent variable and 'y' as the dependent variable\n",
        "\n",
        "# 1. Prepare the data\n",
        "X = df[['x']]  # Assuming 'x' is the column name of your independent variable, changed 'X' to 'x'\n",
        "y = df['y']    # Assuming 'y' is the column name of your dependent variable\n",
        "\n",
        "# 2. Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "# 3. Create polynomial features\n",
        "poly = PolynomialFeatures(degree=2)  # Example: 2nd degree polynomial\n",
        "X_train_poly = poly.fit_transform(X_train)\n",
        "X_test_poly = poly.transform(X_test)\n",
        "\n",
        "# 4. Fit a linear regression model to the polynomial features\n",
        "lin_reg_2 = LinearRegression()\n",
        "lin_reg_2.fit(X_train_poly, y_train)\n",
        "\n",
        "# 5. Make predictions\n",
        "y_pred = lin_reg_2.predict(X_test_poly)\n",
        "\n",
        "# 6. Evaluate the model (optional)\n",
        "# You can calculate metrics like R-squared, MSE, etc., to assess the model's performance.\n",
        "# Example:\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(f\"R-squared: {r2}\")\n",
        "\n",
        "\n",
        "# 7. Visualize the results (optional)\n",
        "plt.scatter(X, y, color='red')\n",
        "plt.plot(X_test, y_pred, color='blue')\n",
        "plt.title('Polynomial Regression')\n",
        "plt.xlabel('x') # Changed 'X' to 'x'\n",
        "plt.ylabel('y')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
